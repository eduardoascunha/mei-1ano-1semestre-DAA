{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados e Aprendizagem Automática\n",
    "### Part VIII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi Layer Percepreton using Titanic Dataset - III**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports, installations and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, Sigmoid, ReLU, Softmax, Module\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "from torch.nn.init import xavier_uniform_, kaiming_uniform_\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset():\n",
    "    def __init__(self, ):\n",
    "\n",
    "        df_X = pd.read_csv(\"titanic_X_scaled.csv\", header=0)\n",
    "        df_y = pd.read_csv(\"titanic_y_scaled.csv\", header=0)\n",
    "\n",
    "        self.X = df_X.values\n",
    "        self.y = df_y.values[:, 0]-1\n",
    "\n",
    "        self.X = self.X.astype('float32')\n",
    "        self.y = torch.tensor(self.y, dtype=torch.long, device=device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    def get_splits(self, n_test):\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])\n",
    "    \n",
    "def prepare_data(n_test):\n",
    "    dataset = CSVDataset()\n",
    "    train, test = dataset.get_splits(n_test)\n",
    "    train_dl = DataLoader(train, batch_size=len(train), shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=len(train), shuffle=True)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = prepare_data(0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 250\n",
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_3(Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP_3, self).__init__()\n",
    "        self.hidden1 = Linear(n_inputs, 24)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        self.hidden2 = Linear(24,40 )\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        self.hidden3 = Linear(40, 32)\n",
    "        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n",
    "        self.act3 = ReLU()\n",
    "        self.hidden4 = Linear(32, 16)\n",
    "        kaiming_uniform_(self.hidden4.weight, nonlinearity='relu')\n",
    "        self.act4 = ReLU()\n",
    "        self.hidden5 = Linear(16, 8)\n",
    "        kaiming_uniform_(self.hidden5.weight, nonlinearity='relu')\n",
    "        self.act5 = ReLU()\n",
    "        self.hidden6 = Linear(8, 3)\n",
    "        xavier_uniform_(self.hidden6.weight)\n",
    "        self.act6 = Softmax(dim=1)\n",
    " \n",
    "    def forward(self, X):\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        X = self.hidden4(X)\n",
    "        X = self.act4(X)\n",
    "        X = self.hidden5(X)\n",
    "        X = self.act5(X)\n",
    "        X = self.hidden6(X)\n",
    "        X = self.act6(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_3(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MLP_3                                    [597, 3]                  --\n",
      "├─Linear: 1-1                            [597, 24]                 216\n",
      "├─ReLU: 1-2                              [597, 24]                 --\n",
      "├─Linear: 1-3                            [597, 40]                 1,000\n",
      "├─ReLU: 1-4                              [597, 40]                 --\n",
      "├─Linear: 1-5                            [597, 32]                 1,312\n",
      "├─ReLU: 1-6                              [597, 32]                 --\n",
      "├─Linear: 1-7                            [597, 16]                 528\n",
      "├─ReLU: 1-8                              [597, 16]                 --\n",
      "├─Linear: 1-9                            [597, 8]                  136\n",
      "├─ReLU: 1-10                             [597, 8]                  --\n",
      "├─Linear: 1-11                           [597, 3]                  27\n",
      "├─Softmax: 1-12                          [597, 3]                  --\n",
      "==========================================================================================\n",
      "Total params: 3,219\n",
      "Trainable params: 3,219\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 1.92\n",
      "==========================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.59\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.62\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP_3(\n",
       "  (hidden1): Linear(in_features=8, out_features=24, bias=True)\n",
       "  (act1): ReLU()\n",
       "  (hidden2): Linear(in_features=24, out_features=40, bias=True)\n",
       "  (act2): ReLU()\n",
       "  (hidden3): Linear(in_features=40, out_features=32, bias=True)\n",
       "  (act3): ReLU()\n",
       "  (hidden4): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (act4): ReLU()\n",
       "  (hidden5): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (act5): ReLU()\n",
       "  (hidden6): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (act6): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(summary(model, input_size=(len(train_dl.dataset), 8), verbose=0))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 settings: Epochs=, LR=0., Softmax-6, CEL, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, test_dl, model):\n",
    "    liveloss = PlotLosses()\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        logs = {}\n",
    "        model.train()\n",
    "        running_loss  = 0.0\n",
    "        running_corrects  = 0.0\n",
    "        for inputs, labels in train_dl:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.detach() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_dl.dataset)\n",
    "        epoch_acc = running_corrects.float() / len(train_dl.dataset)\n",
    "        logs['loss'] = epoch_loss.item()\n",
    "        logs['accuracy'] = epoch_acc.item()\n",
    "\n",
    "        model.eval()\n",
    "        running_loss  = 0.0\n",
    "        running_corrects  = 0.0\n",
    "        for inputs, labels in test_dl: \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.detach() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss = running_loss / len(test_dl.dataset)\n",
    "        epoch_acc = running_corrects.float() / len(test_dl.dataset)\n",
    "        logs['val_loss'] = epoch_loss.item()\n",
    "        logs['val_accuracy'] = epoch_acc.item()\n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_dl, test_dl, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_dl, model):\n",
    "    predictions = list()\n",
    "    actual_values = list()\n",
    "    for i, (inputs, labels) in enumerate(test_dl):\n",
    "        yprev = model(inputs)\n",
    "        yprev = yprev.detach().numpy()\n",
    "        actual = labels.numpy()\n",
    "        yprev = np.argmax(yprev, axis=1)\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        yprev = yprev.reshape((len(yprev), 1))\n",
    "        predictions.append(yprev)\n",
    "        actual_values.append(actual)\n",
    "        break\n",
    "    predictions, actual_values = np.vstack(predictions), np.vstack(actual_values)\n",
    "    return predictions, actual_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(cm):\n",
    "    plt.figure(figsize = (16,8))\n",
    "    sns.heatmap(cm,annot=True,xticklabels=['Class 1', 'Class 2', 'Class 3'], \n",
    "                yticklabels=['Class 1', 'Class 2', 'Class 3'], annot_kws={\"size\": 12}, fmt='g', linewidths=.5)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, actual_values = evaluate_model(test_dl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "failure = 0\n",
    "for r,p in zip(actual_values, predictions):\n",
    "    if r==p: success+=1  \n",
    "    else: failure+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.265\n",
      "\n",
      "success:78 failure:216\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(actual_values, predictions)\n",
    "print(f'Accuracy: {acc:0.3f}\\n')\n",
    "print(f'success:{success} failure:{failure}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      1.00      0.42        78\n",
      "           1       0.00      0.00      0.00        67\n",
      "           2       0.00      0.00      0.00       149\n",
      "\n",
      "    accuracy                           0.27       294\n",
      "   macro avg       0.09      0.33      0.14       294\n",
      "weighted avg       0.07      0.27      0.11       294\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\al76413\\AppData\\Local\\miniconda3\\envs\\conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\al76413\\AppData\\Local\\miniconda3\\envs\\conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\al76413\\AppData\\Local\\miniconda3\\envs\\conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
